{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7861d40-54a7-4496-a7d8-63492307e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample number of Train data:  28709\n",
      "The sample number of Valid data:  3589\n",
      "The sample number of Test data:  3589\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 数据给的是csv文件,其中的表情数据并没有直接给图片，而是给了像素值，没关系，整理的时候顺便转换成图片就好\n",
    "train_d = pd.read_csv(\"./data/train.csv\")\n",
    "val_d = pd.read_csv(\"./data/val.csv\")\n",
    "test_d = pd.read_csv(\"./data/test.csv\")\n",
    "print(\"The sample number of Train data: \", train_d.shape[0])\n",
    "print(\"The sample number of Valid data: \", val_d.shape[0])\n",
    "print(\"The sample number of Test data: \", test_d.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6d59a4-fe56-42e0-9282-e6dc1b67599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "class FaceExpressionDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.data.iloc[idx, 0]\n",
    "        pixels = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape(48, 48)\n",
    "        img = Image.fromarray(pixels, 'L')  # L 表示灰度图\n",
    "        label = self.data.iloc[idx, 0]  # 假设第一列是标签\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "# 数据预处理\n",
    "transform = Compose([\n",
    "    Resize((8, 8)),  # 可以根据需要调整图像尺寸\n",
    "    ToTensor(),  # 转换为Tensor\n",
    "    Normalize(mean=[0.5], std=[0.5])  # 灰度图的简单归一化\n",
    "])\n",
    "# 构建数据集实例\n",
    "train_dataset = FaceExpressionDataset(csv_file=\"./data/train.csv\", root_dir='./data', transform=transform)\n",
    "val_dataset = FaceExpressionDataset(csv_file=\"./data/val.csv\", root_dir='./data', transform=transform)\n",
    "test_dataset = FaceExpressionDataset(csv_file=\"./data/test.csv\", root_dir='./data', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd31cf10-c083-4f4b-b797-ffc069992543",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgray()\n\u001b[1;32m----> 6\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mX_train\u001b[49m[i])\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 各种表情种类的分布\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab308cc6-75a6-4cbe-a670-eaa8556845ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 28709\n",
      "Validation samples: 3589\n",
      "Test samples: 3589\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (48,48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m num_to_show \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     60\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mnum_to_show, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 61\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 将Tensor的维度从(batch_size, channels, height, width)转换为(batch_size, height, width, channels)以用于matplotlib\u001b[39;00m\n\u001b[0;32m     64\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\deep-learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[8], line 42\u001b[0m, in \u001b[0;36mEmotionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 读取像素值并转换为图片\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 42\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpixels_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 应用转换（如果需要）\u001b[39;00m\n\u001b[0;32m     44\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mpixels_to_image\u001b[1;34m(pixels, img_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpixels_to_image\u001b[39m(pixels, img_size):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# 假设pixels是一个一维数组，我们需要将其reshape为二维的图片数组\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     pixels \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 将像素值归一化到0-255范围\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     pixels \u001b[38;5;241m=\u001b[39m (pixels \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (48,48)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # 确保你安装了正确的库，可能是 seaborn 或者直接使用 matplotlib\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 假设CSV文件的格式是：第一列是标签，后面的列是像素值，按照像素的排列顺序\n",
    "\n",
    "# 定义一个函数来将像素值转换为图片\n",
    "def pixels_to_image(pixels, img_size):\n",
    "    # 假设pixels是一个一维数组，我们需要将其reshape为二维的图片数组\n",
    "    pixels = np.array(pixels).reshape(img_size, img_size)\n",
    "    # 将像素值归一化到0-255范围\n",
    "    pixels = (pixels * 255).astype(np.uint8)\n",
    "    # 创建一个PIL Image对象\n",
    "    img = Image.fromarray(pixels)\n",
    "    return img\n",
    "\n",
    "# 定义一个自定义Dataset类\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_size=48):  # 假设图片大小是48x48，你可以根据实际需要调整\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.ToTensor()  # 转换为Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 读取标签\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        # 读取像素值并转换为图片\n",
    "        pixels = self.data.iloc[idx, 1:].values\n",
    "        image = pixels_to_image(pixels, self.img_size)\n",
    "        # 应用转换（如果需要）\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = EmotionDataset(\"./data/train.csv\")\n",
    "val_dataset = EmotionDataset(\"./data/val.csv\")\n",
    "test_dataset = EmotionDataset(\"./data/test.csv\")\n",
    "\n",
    "# 打印数据集样本数\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))\n",
    "\n",
    "# 数据展示\n",
    "# 选择一些图片进行展示\n",
    "num_to_show = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=num_to_show, shuffle=True)\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.permute(1, 2, 0)  # 将Tensor的维度从(batch_size, channels, height, width)转换为(batch_size, height, width, channels)以用于matplotlib\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(num_to_show):\n",
    "    plt.subplot(1, num_to_show, i+1)\n",
    "    plt.imshow(images[i].numpy().squeeze(), cmap='gray')  # 假设图片是灰度图\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "plt.show()\n",
    "\n",
    "# 生成表情种类分布图\n",
    "# 假设标签是整数，并且从0开始\n",
    "train_labels = train_dataset.data.iloc[:, 0].values\n",
    "val_labels = val_dataset.data.iloc[:, 0].values\n",
    "test_labels = test_dataset.data.iloc[:, 0].values\n",
    "\n",
    "# 合并所有标签以计算分布\n",
    "all_labels = np.concatenate([train_labels])\n",
    "# 展示样本图像（如果你已经知道图像是彩色的，需要相应地调整imshow的参数）\n",
    "num_to_show = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=num_to_show, shuffle=True)\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.numpy().transpose(0, 2, 3, 1)  # 将Tensor转换为numpy数组并调整维度以匹配matplotlib的imshow\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_to_show):\n",
    "    plt.subplot(1, num_to_show, i + 1)\n",
    "    plt.imshow(images[i], cmap='gray')  # 假设是灰度图像\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "plt.show()\n",
    "\n",
    "# 计算表情种类分布\n",
    "# 合并所有标签并计算每个类别的数量\n",
    "all_labels = np.concatenate([train_dataset.data.iloc[:, 0].values,\n",
    "                              val_dataset.data.iloc[:, 0].values,\n",
    "                              test_dataset.data.iloc[:, 0].values])\n",
    "label_counts = pd.value_counts(all_labels)\n",
    "\n",
    "# 生成分布图（这里使用柱状图）\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Emotion Distribution')\n",
    "plt.xlabel('Emotion Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)  # 如果标签名字太长，可以旋转x轴标签以便更好地显示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce08a37-d0cb-4ecc-9608-da7b3fa293a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
